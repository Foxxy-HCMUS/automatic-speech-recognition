{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fist we need define the video ids in the `data/video_ids.txt` file. The file should contain one video id per line. For example:\n",
    "\n",
    "```\n",
    "video_id_1\n",
    "video_id_2\n",
    "...\n",
    "video_id_n\n",
    "```\n",
    "\n",
    "This can be done by manually creating the file or by define the variable `VIDEO_SEARCH_START_DATE` in `src/config.py` file. The `VIDEO_SEARCH_START_DATE` should be in the datetime data type `datetime(YYYY, MM, DD)` and the script will search for all videos uploaded 15 days after this date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the following command to download the video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !python src/asr/utils/collect_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will download all the audio files from the videos in the `data/video_ids.txt` file and save them in the `data/raw/audios` folder, with the name `video_id.mp3`. The subtitles will be saved in the `data/raw/subtitles` folder with the name `video_id.vtt`. The metadata will be saved in the `data/metadata` folder with the name `video_id.json`.\n",
    "\n",
    "The metadata will contain the following information:\n",
    "- `video_id`: the video id\n",
    "- `title`: the video title\n",
    "- `description`: the video description\n",
    "- `tags`: the video tags\n",
    "- `category`: the video category\n",
    "- `duration`: the video duration\n",
    "\n",
    "The subtitles will be saved in the VTT format. The VTT format is a simple text format that contains the subtitles in the following format:\n",
    "\n",
    "```vtt\n",
    "WEBVTT\n",
    "Kind: captions\n",
    "Language: en\n",
    "\n",
    "00:00:04.376 --> 00:00:08.463\n",
    "My first job as an investor\n",
    "was when I was 24 years old,\n",
    "\n",
    "00:00:08.505 --> 00:00:12.217\n",
    "and I'm almost 50 today,\n",
    "so that is half a lifetime ago.\n",
    "\n",
    "...\n",
    "```\n",
    "Alternatively, the script will save subtitle files in the original format provided by the author if available. In cases where the author's subtitles are not accessible, we will utilize automatic captions from YouTube as a substitute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the data we will preprocess the audio and subtitles. The subtitles will be converted to a json file, where each subtitle will be a dictionary with the following keys:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"speaker\": \"A\",\n",
    "            \"text\": \" Directions, in this part, you will be asked to refer to information on the screen in order to answer three questions.\",\n",
    "            \"start\": 0,\n",
    "            \"end\": 1020 // in milliseconds\n",
    "        },\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"speaker\": \"A\",\n",
    "            \"text\": \" Directions, in this part, you will be asked to refer to information on the screen in order to answer three questions.\",\n",
    "            \"start\": 2003,\n",
    "            \"end\": 10200 // in milliseconds\n",
    "        }\n",
    "        // ....\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161 overall subtitles\n",
      "1161 without overlap subtitles\n",
      "1161 after filtering\n",
      "83 merged\n",
      "not cool \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from asr.utils.parser import parse_subtitle\n",
    "import config as cfg\n",
    "\n",
    "subtitles = parse_subtitle(f\"{cfg.SUBTITLE_RAW_PATH}/{cfg.TWO_PEOPLE_VIDEO_ID}-en-auto.vtt\")\n",
    "with open(f\"{cfg.SUBTITLE_PROCESSED_PATH}/{cfg.TWO_PEOPLE_VIDEO_ID}.json\", \"w+\") as f:\n",
    "    json.dump(subtitles, f, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_start</th>\n",
       "      <th>ts_end</th>\n",
       "      <th>original_phrase</th>\n",
       "      <th>sub_file</th>\n",
       "      <th>duration</th>\n",
       "      <th>idx</th>\n",
       "      <th>phrase</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00.080000</td>\n",
       "      <td>00:00:14.719000</td>\n",
       "      <td>let's say that you are designing a let's say...</td>\n",
       "      <td>/space/hotel/phit/personal/asr/data/raw/subtit...</td>\n",
       "      <td>14.639</td>\n",
       "      <td>0</td>\n",
       "      <td>let's say that you are designing a</td>\n",
       "      <td>065479dc321578faf12de4c03b5082b80893134e9363c2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:14.719000</td>\n",
       "      <td>00:00:28.800000</td>\n",
       "      <td>marketplace is selling a gun how would you go ...</td>\n",
       "      <td>/space/hotel/phit/personal/asr/data/raw/subtit...</td>\n",
       "      <td>14.081</td>\n",
       "      <td>16</td>\n",
       "      <td>marketplace is selling a gun how would you go ...</td>\n",
       "      <td>d54cb81716f422937117c190414326a5bc946f56cb8912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:28.800000</td>\n",
       "      <td>00:00:43.680000</td>\n",
       "      <td>listings what happens with those identificatio...</td>\n",
       "      <td>/space/hotel/phit/personal/asr/data/raw/subtit...</td>\n",
       "      <td>14.880</td>\n",
       "      <td>32</td>\n",
       "      <td>listings what happens with those identificatio...</td>\n",
       "      <td>02581d6c4b20027dcab817fe4fe5260753f2fd8cea1e01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:43.680000</td>\n",
       "      <td>00:00:57.360000</td>\n",
       "      <td>and then a user can flag the listing if they s...</td>\n",
       "      <td>/space/hotel/phit/personal/asr/data/raw/subtit...</td>\n",
       "      <td>13.680</td>\n",
       "      <td>44</td>\n",
       "      <td>and then a user can flag the listing if they s...</td>\n",
       "      <td>0748072ad938406083885a698ffe51336765456031604c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:00:57.360000</td>\n",
       "      <td>00:01:12.159000</td>\n",
       "      <td>determine it as a gun and that's the only thin...</td>\n",
       "      <td>/space/hotel/phit/personal/asr/data/raw/subtit...</td>\n",
       "      <td>14.799</td>\n",
       "      <td>58</td>\n",
       "      <td>determine it as a gun and that's the only thin...</td>\n",
       "      <td>0f925ccf7ff103e2d8dd13b28e468be09571e3992d84e9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts_start           ts_end  \\\n",
       "0  00:00:00.080000  00:00:14.719000   \n",
       "1  00:00:14.719000  00:00:28.800000   \n",
       "2  00:00:28.800000  00:00:43.680000   \n",
       "3  00:00:43.680000  00:00:57.360000   \n",
       "4  00:00:57.360000  00:01:12.159000   \n",
       "\n",
       "                                     original_phrase  \\\n",
       "0    let's say that you are designing a let's say...   \n",
       "1  marketplace is selling a gun how would you go ...   \n",
       "2  listings what happens with those identificatio...   \n",
       "3  and then a user can flag the listing if they s...   \n",
       "4  determine it as a gun and that's the only thin...   \n",
       "\n",
       "                                            sub_file  duration  idx  \\\n",
       "0  /space/hotel/phit/personal/asr/data/raw/subtit...    14.639    0   \n",
       "1  /space/hotel/phit/personal/asr/data/raw/subtit...    14.081   16   \n",
       "2  /space/hotel/phit/personal/asr/data/raw/subtit...    14.880   32   \n",
       "3  /space/hotel/phit/personal/asr/data/raw/subtit...    13.680   44   \n",
       "4  /space/hotel/phit/personal/asr/data/raw/subtit...    14.799   58   \n",
       "\n",
       "                                              phrase  \\\n",
       "0                 let's say that you are designing a   \n",
       "1  marketplace is selling a gun how would you go ...   \n",
       "2  listings what happens with those identificatio...   \n",
       "3  and then a user can flag the listing if they s...   \n",
       "4  determine it as a gun and that's the only thin...   \n",
       "\n",
       "                                                hash  \n",
       "0  065479dc321578faf12de4c03b5082b80893134e9363c2...  \n",
       "1  d54cb81716f422937117c190414326a5bc946f56cb8912...  \n",
       "2  02581d6c4b20027dcab817fe4fe5260753f2fd8cea1e01...  \n",
       "3  0748072ad938406083885a698ffe51336765456031604c...  \n",
       "4  0f925ccf7ff103e2d8dd13b28e468be09571e3992d84e9...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(subtitles)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/m-bain/whisperX.git@78dcfaab51005aa703ee21375f81ed31bc248560\n",
    "# !pip install dora-search lameenc openunmix wget Cython\n",
    "# !pip install --no-build-isolation \"nemo_toolkit[asr]==1.23.0\"\n",
    "# !pip install --no-deps git+https://github.com/facebookresearch/demucs#egg=demucs\n",
    "# !pip install git+https://github.com/oliverguhr/deepmultilingualpunctuation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This version of torchaudio is old. SpeechBrain no longer tries using the torchaudio global backend mechanism in recipes, so if you encounter issues, update torchaudio.\n",
      "This version of torchaudio is old. SpeechBrain no longer tries using the torchaudio global backend mechanism in recipes, so if you encounter issues, update torchaudio.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "from omegaconf import OmegaConf\n",
    "import json\n",
    "import shutil\n",
    "import whisperx\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "import re\n",
    "import logging\n",
    "import nltk\n",
    "from whisperx.alignment import DEFAULT_ALIGN_MODELS_HF, DEFAULT_ALIGN_MODELS_TORCH\n",
    "from whisperx.utils import LANGUAGES, TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "punct_model_langs = [\n",
    "    \"en\",\n",
    "    \"fr\",\n",
    "    \"de\",\n",
    "    \"es\",\n",
    "    \"it\",\n",
    "    \"nl\",\n",
    "    \"pt\",\n",
    "    \"bg\",\n",
    "    \"pl\",\n",
    "    \"cs\",\n",
    "    \"sk\",\n",
    "    \"sl\",\n",
    "]\n",
    "wav2vec2_langs = list(DEFAULT_ALIGN_MODELS_TORCH.keys()) + list(\n",
    "    DEFAULT_ALIGN_MODELS_HF.keys()\n",
    ")\n",
    "\n",
    "whisper_langs = sorted(LANGUAGES.keys()) + sorted(\n",
    "    [k.title() for k in TO_LANGUAGE_CODE.keys()]\n",
    ")\n",
    "\n",
    "\n",
    "def create_config(output_dir):\n",
    "    DOMAIN_TYPE = \"telephonic\"  # Can be meeting, telephonic, or general based on domain type of the audio file\n",
    "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
    "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
    "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
    "    if not os.path.exists(MODEL_CONFIG):\n",
    "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)\n",
    "\n",
    "    config = OmegaConf.load(MODEL_CONFIG)\n",
    "\n",
    "    data_dir = os.path.join(output_dir, \"data\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    meta = {\n",
    "        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n",
    "        \"offset\": 0,\n",
    "        \"duration\": None,\n",
    "        \"label\": \"infer\",\n",
    "        \"text\": \"-\",\n",
    "        \"rttm_filepath\": None,\n",
    "        \"uem_filepath\": None,\n",
    "    }\n",
    "    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n",
    "        json.dump(meta, fp)\n",
    "        fp.write(\"\\n\")\n",
    "\n",
    "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
    "    pretrained_speaker_model = \"titanet_large\"\n",
    "    config.num_workers = 0  # Workaround for multiprocessing hanging with ipython issue\n",
    "    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n",
    "    config.diarizer.out_dir = (\n",
    "        output_dir  # Directory to store intermediate files and prediction outputs\n",
    "    )\n",
    "\n",
    "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
    "    config.diarizer.oracle_vad = (\n",
    "        False  # compute VAD provided with model_path to vad config\n",
    "    )\n",
    "    config.diarizer.clustering.parameters.oracle_num_speakers = False\n",
    "\n",
    "    # Here, we use our in-house pretrained NeMo VAD model\n",
    "    config.diarizer.vad.model_path = pretrained_vad\n",
    "    config.diarizer.vad.parameters.onset = 0.8\n",
    "    config.diarizer.vad.parameters.offset = 0.6\n",
    "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
    "    config.diarizer.msdd_model.model_path = (\n",
    "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
    "    )\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_word_ts_anchor(s, e, option=\"start\"):\n",
    "    if option == \"end\":\n",
    "        return e\n",
    "    elif option == \"mid\":\n",
    "        return (s + e) / 2\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
    "    s, e, sp = spk_ts[0]\n",
    "    wrd_pos, turn_idx = 0, 0\n",
    "    wrd_spk_mapping = []\n",
    "    for wrd_dict in wrd_ts:\n",
    "        ws, we, wrd = (\n",
    "            int(wrd_dict[\"start\"] * 1000),\n",
    "            int(wrd_dict[\"end\"] * 1000),\n",
    "            wrd_dict[\"word\"],\n",
    "        )\n",
    "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
    "        while wrd_pos > float(e):\n",
    "            turn_idx += 1\n",
    "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
    "            s, e, sp = spk_ts[turn_idx]\n",
    "            if turn_idx == len(spk_ts) - 1:\n",
    "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
    "        wrd_spk_mapping.append(\n",
    "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
    "        )\n",
    "    return wrd_spk_mapping\n",
    "\n",
    "\n",
    "sentence_ending_punctuations = \".?!\"\n",
    "\n",
    "\n",
    "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
    "    is_word_sentence_end = (\n",
    "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
    "    )\n",
    "    left_idx = word_idx\n",
    "    while (\n",
    "        left_idx > 0\n",
    "        and word_idx - left_idx < max_words\n",
    "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
    "        and not is_word_sentence_end(left_idx - 1)\n",
    "    ):\n",
    "        left_idx -= 1\n",
    "\n",
    "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
    "\n",
    "\n",
    "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
    "    is_word_sentence_end = (\n",
    "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
    "    )\n",
    "    right_idx = word_idx\n",
    "    while (\n",
    "        right_idx < len(word_list)\n",
    "        and right_idx - word_idx < max_words\n",
    "        and not is_word_sentence_end(right_idx)\n",
    "    ):\n",
    "        right_idx += 1\n",
    "\n",
    "    return (\n",
    "        right_idx\n",
    "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
    "        else -1\n",
    "    )\n",
    "\n",
    "\n",
    "def get_realigned_ws_mapping_with_punctuation(\n",
    "    word_speaker_mapping, max_words_in_sentence=50\n",
    "):\n",
    "    is_word_sentence_end = (\n",
    "        lambda x: x >= 0\n",
    "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
    "    )\n",
    "    wsp_len = len(word_speaker_mapping)\n",
    "\n",
    "    words_list, speaker_list = [], []\n",
    "    for k, line_dict in enumerate(word_speaker_mapping):\n",
    "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
    "        words_list.append(word)\n",
    "        speaker_list.append(speaker)\n",
    "\n",
    "    k = 0\n",
    "    while k < len(word_speaker_mapping):\n",
    "        line_dict = word_speaker_mapping[k]\n",
    "        if (\n",
    "            k < wsp_len - 1\n",
    "            and speaker_list[k] != speaker_list[k + 1]\n",
    "            and not is_word_sentence_end(k)\n",
    "        ):\n",
    "            left_idx = get_first_word_idx_of_sentence(\n",
    "                k, words_list, speaker_list, max_words_in_sentence\n",
    "            )\n",
    "            right_idx = (\n",
    "                get_last_word_idx_of_sentence(\n",
    "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
    "                )\n",
    "                if left_idx > -1\n",
    "                else -1\n",
    "            )\n",
    "            if min(left_idx, right_idx) == -1:\n",
    "                k += 1\n",
    "                continue\n",
    "\n",
    "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
    "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
    "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
    "                k += 1\n",
    "                continue\n",
    "\n",
    "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
    "                right_idx - left_idx + 1\n",
    "            )\n",
    "            k = right_idx\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    k, realigned_list = 0, []\n",
    "    while k < len(word_speaker_mapping):\n",
    "        line_dict = word_speaker_mapping[k].copy()\n",
    "        line_dict[\"speaker\"] = speaker_list[k]\n",
    "        realigned_list.append(line_dict)\n",
    "        k += 1\n",
    "\n",
    "    return realigned_list\n",
    "\n",
    "\n",
    "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
    "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
    "    s, e, spk = spk_ts[0]\n",
    "    prev_spk = spk\n",
    "\n",
    "    snts = []\n",
    "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
    "\n",
    "    for wrd_dict in word_speaker_mapping:\n",
    "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
    "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
    "        if spk != prev_spk or sentence_checker(snt[\"text\"] + \" \" + wrd):\n",
    "            snts.append(snt)\n",
    "            snt = {\n",
    "                \"speaker\": f\"Speaker {spk}\",\n",
    "                \"start_time\": s,\n",
    "                \"end_time\": e,\n",
    "                \"text\": \"\",\n",
    "            }\n",
    "        else:\n",
    "            snt[\"end_time\"] = e\n",
    "        snt[\"text\"] += wrd + \" \"\n",
    "        prev_spk = spk\n",
    "\n",
    "    snts.append(snt)\n",
    "    return snts\n",
    "\n",
    "\n",
    "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
    "    previous_speaker = sentences_speaker_mapping[0][\"speaker\"]\n",
    "    f.write(f\"{previous_speaker}: \")\n",
    "\n",
    "    for sentence_dict in sentences_speaker_mapping:\n",
    "        speaker = sentence_dict[\"speaker\"]\n",
    "        sentence = sentence_dict[\"text\"]\n",
    "\n",
    "        # If this speaker doesn't match the previous one, start a new paragraph\n",
    "        if speaker != previous_speaker:\n",
    "            f.write(f\"\\n\\n{speaker}: \")\n",
    "            previous_speaker = speaker\n",
    "\n",
    "        # No matter what, write the current sentence\n",
    "        f.write(sentence + \" \")\n",
    "\n",
    "\n",
    "def format_timestamp(\n",
    "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
    "):\n",
    "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
    "\n",
    "    hours = milliseconds // 3_600_000\n",
    "    milliseconds -= hours * 3_600_000\n",
    "\n",
    "    minutes = milliseconds // 60_000\n",
    "    milliseconds -= minutes * 60_000\n",
    "\n",
    "    seconds = milliseconds // 1_000\n",
    "    milliseconds -= seconds * 1_000\n",
    "\n",
    "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
    "    return (\n",
    "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def write_srt(transcript, file):\n",
    "    \"\"\"\n",
    "    Write a transcript to a file in SRT format.\n",
    "\n",
    "    \"\"\"\n",
    "    for i, segment in enumerate(transcript, start=1):\n",
    "        # write srt lines\n",
    "        print(\n",
    "            f\"{i}\\n\"\n",
    "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
    "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
    "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
    "            file=file,\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "\n",
    "def find_numeral_symbol_tokens(tokenizer):\n",
    "    numeral_symbol_tokens = [\n",
    "        -1,\n",
    "    ]\n",
    "    for token, token_id in tokenizer.get_vocab().items():\n",
    "        has_numeral_symbol = any(c in \"0123456789%$£\" for c in token)\n",
    "        if has_numeral_symbol:\n",
    "            numeral_symbol_tokens.append(token_id)\n",
    "    return numeral_symbol_tokens\n",
    "\n",
    "\n",
    "def _get_next_start_timestamp(word_timestamps, current_word_index, final_timestamp):\n",
    "    # if current word is the last word\n",
    "    if current_word_index == len(word_timestamps) - 1:\n",
    "        return word_timestamps[current_word_index][\"start\"]\n",
    "\n",
    "    next_word_index = current_word_index + 1\n",
    "    while current_word_index < len(word_timestamps) - 1:\n",
    "        if word_timestamps[next_word_index].get(\"start\") is None:\n",
    "            # if next word doesn't have a start timestamp\n",
    "            # merge it with the current word and delete it\n",
    "            word_timestamps[current_word_index][\"word\"] += (\n",
    "                \" \" + word_timestamps[next_word_index][\"word\"]\n",
    "            )\n",
    "\n",
    "            word_timestamps[next_word_index][\"word\"] = None\n",
    "            next_word_index += 1\n",
    "            if next_word_index == len(word_timestamps):\n",
    "                return final_timestamp\n",
    "\n",
    "        else:\n",
    "            return word_timestamps[next_word_index][\"start\"]\n",
    "\n",
    "\n",
    "def filter_missing_timestamps(\n",
    "    word_timestamps, initial_timestamp=0, final_timestamp=None\n",
    "):\n",
    "    # handle the first and last word\n",
    "    if word_timestamps[0].get(\"start\") is None:\n",
    "        word_timestamps[0][\"start\"] = (\n",
    "            initial_timestamp if initial_timestamp is not None else 0\n",
    "        )\n",
    "        word_timestamps[0][\"end\"] = _get_next_start_timestamp(\n",
    "            word_timestamps, 0, final_timestamp\n",
    "        )\n",
    "\n",
    "    result = [\n",
    "        word_timestamps[0],\n",
    "    ]\n",
    "\n",
    "    for i, ws in enumerate(word_timestamps[1:], start=1):\n",
    "        # if ws doesn't have a start and end\n",
    "        # use the previous end as start and next start as end\n",
    "        if ws.get(\"start\") is None and ws.get(\"word\") is not None:\n",
    "            ws[\"start\"] = word_timestamps[i - 1][\"end\"]\n",
    "            ws[\"end\"] = _get_next_start_timestamp(word_timestamps, i, final_timestamp)\n",
    "\n",
    "        if ws[\"word\"] is not None:\n",
    "            result.append(ws)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cleanup(path: str):\n",
    "    \"\"\"path could either be relative or absolute.\"\"\"\n",
    "    # check if file or directory exists\n",
    "    if os.path.isfile(path) or os.path.islink(path):\n",
    "        # remove file\n",
    "        os.remove(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # remove directory and all its content\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        raise ValueError(\"Path {} is not a file or dir.\".format(path))\n",
    "\n",
    "\n",
    "def process_language_arg(language: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Process the language argument to make sure it's valid and convert language names to language codes.\n",
    "    \"\"\"\n",
    "    if language is not None:\n",
    "        language = language.lower()\n",
    "    if language not in LANGUAGES:\n",
    "        if language in TO_LANGUAGE_CODE:\n",
    "            language = TO_LANGUAGE_CODE[language]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported language: {language}\")\n",
    "\n",
    "    if model_name.endswith(\".en\") and language != \"en\":\n",
    "        if language is not None:\n",
    "            logging.warning(\n",
    "                f\"{model_name} is an English-only model but received '{language}'; using English instead.\"\n",
    "            )\n",
    "        language = \"en\"\n",
    "    return language\n",
    "\n",
    "\n",
    "def transcribe(\n",
    "    audio_file: str,\n",
    "    language: str,\n",
    "    model_name: str,\n",
    "    compute_dtype: str,\n",
    "    suppress_numerals: bool,\n",
    "    device: str,\n",
    "):\n",
    "    from faster_whisper import WhisperModel\n",
    "    from helpers import find_numeral_symbol_tokens, wav2vec2_langs\n",
    "\n",
    "    # Faster Whisper non-batched\n",
    "    # Run on GPU with FP16\n",
    "    whisper_model = WhisperModel(model_name, device=device, compute_type=compute_dtype)\n",
    "\n",
    "    # or run on GPU with INT8\n",
    "    # model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "    # or run on CPU with INT8\n",
    "    # model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "    if suppress_numerals:\n",
    "        numeral_symbol_tokens = find_numeral_symbol_tokens(whisper_model.hf_tokenizer)\n",
    "    else:\n",
    "        numeral_symbol_tokens = None\n",
    "\n",
    "    if language is not None and language in wav2vec2_langs:\n",
    "        word_timestamps = False\n",
    "    else:\n",
    "        word_timestamps = True\n",
    "\n",
    "    segments, info = whisper_model.transcribe(\n",
    "        audio_file,\n",
    "        language=language,\n",
    "        beam_size=5,\n",
    "        word_timestamps=word_timestamps,  # TODO: disable this if the language is supported by wav2vec2\n",
    "        suppress_tokens=numeral_symbol_tokens,\n",
    "        vad_filter=True,\n",
    "    )\n",
    "    whisper_results = []\n",
    "    for segment in segments:\n",
    "        whisper_results.append(segment._asdict())\n",
    "    # clear gpu vram\n",
    "    del whisper_model\n",
    "    torch.cuda.empty_cache()\n",
    "    return whisper_results, language\n",
    "\n",
    "\n",
    "def transcribe_batched(\n",
    "    audio_file: str,\n",
    "    language: str,\n",
    "    batch_size: int,\n",
    "    model_name: str,\n",
    "    compute_dtype: str,\n",
    "    suppress_numerals: bool,\n",
    "    device: str,\n",
    "):\n",
    "    import whisperx\n",
    "\n",
    "    # Faster Whisper batched\n",
    "    whisper_model = whisperx.load_model(\n",
    "        model_name,\n",
    "        device,\n",
    "        compute_type=compute_dtype,\n",
    "        asr_options={\"suppress_numerals\": suppress_numerals},\n",
    "    )\n",
    "    audio = whisperx.load_audio(audio_file)\n",
    "    result = whisper_model.transcribe(audio, language=language, batch_size=batch_size)\n",
    "    del whisper_model\n",
    "    torch.cuda.empty_cache()\n",
    "    return result[\"segments\"], result[\"language\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Name of the audio file\n",
    "audio_path = f\"{cfg.AUDIO_RAW_PATH}/{cfg.ASSESSMENT_VIDEO_IDS[1]}.mp3\"\n",
    "\n",
    "# Whether to enable music removal from speech, helps increase diarization quality but uses alot of ram\n",
    "enable_stemming = True\n",
    "\n",
    "# (choose from 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large-v3', 'large')\n",
    "whisper_model_name = \"large-v3\"\n",
    "\n",
    "# replaces numerical digits with their pronounciation, increases diarization accuracy\n",
    "suppress_numerals = True\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "language = None  # autodetect language\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating music from speech using Demucs\n",
    "---\n",
    "By isolating the vocals from the rest of the audio, it becomes easier to identify and track individual speakers based on the spectral and temporal characteristics of their speech signals. Source separation is just one of many techniques that can be used as a preprocessing step to help improve the accuracy and reliability of the overall diarization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model is a bag of 1 models. You will see that many progress bars per track.\n",
      "Separated tracks will be stored in /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/htdemucs\n",
      "Separating track /space/hotel/phit/personal/asr/data/raw/audios/Y8tlFLIjyMU.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1585.35/1585.35 [00:36<00:00, 43.40seconds/s]\n"
     ]
    }
   ],
   "source": [
    "if enable_stemming:\n",
    "    # Isolate vocals from the rest of the audio\n",
    "\n",
    "    return_code = os.system(\n",
    "        f'python3 -m demucs.separate -n htdemucs --two-stems=vocals \"{audio_path}\" -o \"temp_outputs\"'\n",
    "    )\n",
    "\n",
    "    if return_code != 0:\n",
    "        logging.warning(\"Source splitting failed, using original audio file.\")\n",
    "        vocal_target = audio_path\n",
    "    else:\n",
    "        vocal_target = os.path.join(\n",
    "            \"temp_outputs\",\n",
    "            \"htdemucs\",\n",
    "            os.path.splitext(os.path.basename(audio_path))[0],\n",
    "            \"vocals.wav\",\n",
    "        )\n",
    "else:\n",
    "    vocal_target = audio_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcriping audio using Whisper and realligning timestamps using Wav2Vec2\n",
    "---\n",
    "This code uses two different open-source models to transcribe speech and perform forced alignment on the resulting transcription.\n",
    "\n",
    "The first model is called OpenAI Whisper, which is a speech recognition model that can transcribe speech with high accuracy. The code loads the whisper model and uses it to transcribe the vocal_target file.\n",
    "\n",
    "The output of the transcription process is a set of text segments with corresponding timestamps indicating when each segment was spoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# !pip install ctranslate2==3.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.7. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../../../../../space/hotel/phit/.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Detected language: en (0.98) in first 30s of audio...\n",
      "Suppressing numeral and symbol tokens\n"
     ]
    }
   ],
   "source": [
    "compute_type = \"float16\"\n",
    "# or run on GPU with INT8\n",
    "# compute_type = \"int8_float16\"\n",
    "# or run on CPU with INT8\n",
    "# compute_type = \"int8\"\n",
    "\n",
    "if batch_size != 0:\n",
    "    whisper_results, language = transcribe_batched(\n",
    "        vocal_target,\n",
    "        language,\n",
    "        batch_size,\n",
    "        whisper_model_name,\n",
    "        compute_type,\n",
    "        suppress_numerals,\n",
    "        device,\n",
    "    )\n",
    "else:\n",
    "    whisper_results, language = transcribe(\n",
    "        vocal_target,\n",
    "        language,\n",
    "        whisper_model_name,\n",
    "        compute_type,\n",
    "        suppress_numerals,\n",
    "        device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning the transcription with the original audio using Wav2Vec2\n",
    "---\n",
    "The second model used is called wav2vec2, which is a large-scale neural network that is designed to learn representations of speech that are useful for a variety of speech processing tasks, including speech recognition and alignment.\n",
    "\n",
    "The code loads the wav2vec2 alignment model and uses it to align the transcription segments with the original audio signal contained in the vocal_target file. This process involves finding the exact timestamps in the audio signal where each segment was spoken and aligning the text accordingly.\n",
    "\n",
    "By combining the outputs of the two models, the code produces a fully aligned transcription of the speech contained in the vocal_target file. This aligned transcription can be useful for a variety of speech processing tasks, such as speaker diarization, sentiment analysis, and language identification.\n",
    "\n",
    "If there's no Wav2Vec2 model available for your language, word timestamps generated by whisper will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if language in wav2vec2_langs:\n",
    "    device = \"cuda\"\n",
    "    alignment_model, metadata = whisperx.load_align_model(\n",
    "        language_code=language, device=device\n",
    "    )\n",
    "    result_aligned = whisperx.align(\n",
    "        whisper_results, alignment_model, metadata, vocal_target, device\n",
    "    )\n",
    "    word_timestamps = filter_missing_timestamps(\n",
    "        result_aligned[\"word_segments\"],\n",
    "        initial_timestamp=whisper_results[0].get(\"start\"),\n",
    "        final_timestamp=whisper_results[-1].get(\"end\"),\n",
    "    )\n",
    "\n",
    "    # clear gpu vram\n",
    "    del alignment_model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    assert batch_size == 0, (  # TODO: add a better check for word timestamps existence\n",
    "        f\"Unsupported language: {language}, use --batch_size to 0\"\n",
    "        \" to generate word timestamps using whisper directly and fix this error.\"\n",
    "    )\n",
    "    word_timestamps = []\n",
    "    for segment in whisper_results:\n",
    "        for word in segment[\"words\"]:\n",
    "            word_timestamps.append({\"word\": word[2], \"start\": word[0], \"end\": word[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert audio to mono for NeMo combatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='/mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/mono_file.wav'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound = AudioSegment.from_file(vocal_target).set_channels(1)\n",
    "ROOT = os.getcwd()\n",
    "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
    "os.makedirs(temp_path, exist_ok=True)\n",
    "sound.export(os.path.join(temp_path, \"mono_file.wav\"), format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Diarization using NeMo MSDD Model\n",
    "---\n",
    "This code uses a model called Nvidia NeMo MSDD (Multi-scale Diarization Decoder) to perform speaker diarization on an audio signal. Speaker diarization is the process of separating an audio signal into different segments based on who is speaking at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:03:37 msdd_models:1092] Loading pretrained diar_msdd_telephonic model from NGC\n",
      "[NeMo I 2024-05-11 13:03:37 cloud:58] Found existing object /home/phit/.cache/torch/NeMo/NeMo_1.23.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2024-05-11 13:03:37 cloud:64] Re-using file from: /home/phit/.cache/torch/NeMo/NeMo_1.23.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
      "[NeMo I 2024-05-11 13:03:37 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:03:38 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: true\n",
      "    \n",
      "[NeMo W 2024-05-11 13:03:38 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    \n",
      "[NeMo W 2024-05-11 13:03:38 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    emb_dir: null\n",
      "    sample_rate: 16000\n",
      "    num_spks: 2\n",
      "    soft_label_thres: 0.5\n",
      "    labels: null\n",
      "    batch_size: 15\n",
      "    emb_batch_size: 0\n",
      "    shuffle: false\n",
      "    seq_eval_mode: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:03:39 features:289] PADDING: 16\n",
      "[NeMo I 2024-05-11 13:03:55 features:289] PADDING: 16\n",
      "[NeMo I 2024-05-11 13:03:55 audio_preprocessing:517] Numba CUDA SpecAugment kernel is being used\n",
      "[NeMo I 2024-05-11 13:03:56 save_restore_connector:249] Model EncDecDiarLabelModel was successfully restored from /home/phit/.cache/torch/NeMo/NeMo_1.23.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
      "[NeMo I 2024-05-11 13:03:56 features:289] PADDING: 16\n",
      "[NeMo I 2024-05-11 13:03:56 audio_preprocessing:517] Numba CUDA SpecAugment kernel is being used\n",
      "[NeMo I 2024-05-11 13:03:56 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
      "[NeMo I 2024-05-11 13:03:56 cloud:58] Found existing object /home/phit/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2024-05-11 13:03:56 cloud:64] Re-using file from: /home/phit/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
      "[NeMo I 2024-05-11 13:03:56 common:924] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:03:56 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    augmentor:\n",
      "      shift:\n",
      "        prob: 0.5\n",
      "        min_shift_ms: -10.0\n",
      "        max_shift_ms: 10.0\n",
      "      white_noise:\n",
      "        prob: 0.5\n",
      "        min_level: -90\n",
      "        max_level: -46\n",
      "        norm: true\n",
      "      noise:\n",
      "        prob: 0.5\n",
      "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
      "        min_snr_db: 0\n",
      "        max_snr_db: 30\n",
      "        max_gain_db: 300.0\n",
      "        norm: true\n",
      "      gain:\n",
      "        prob: 0.5\n",
      "        min_gain_dbfs: -10.0\n",
      "        max_gain_dbfs: 10.0\n",
      "        norm: true\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-05-11 13:03:56 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 256\n",
      "    shuffle: false\n",
      "    val_loss_idx: 0\n",
      "    num_workers: 16\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-05-11 13:03:56 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - background\n",
      "    - speech\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    test_loss_idx: 0\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:03:56 features:289] PADDING: 16\n",
      "[NeMo I 2024-05-11 13:03:56 save_restore_connector:249] Model EncDecClassificationModel was successfully restored from /home/phit/.cache/torch/NeMo/NeMo_1.23.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
      "[NeMo I 2024-05-11 13:03:57 msdd_models:864] Multiscale Weights: [1, 1, 1, 1, 1]\n",
      "[NeMo I 2024-05-11 13:03:57 msdd_models:865] Clustering Parameters: {\n",
      "        \"oracle_num_speakers\": false,\n",
      "        \"max_num_speakers\": 8,\n",
      "        \"enhanced_count_thres\": 80,\n",
      "        \"max_rp_threshold\": 0.25,\n",
      "        \"sparse_search_volume\": 30,\n",
      "        \"maj_vote_spk_count\": false,\n",
      "        \"chunk_cluster_count\": 50,\n",
      "        \"embeddings_per_chunk\": 10000\n",
      "    }\n",
      "[NeMo I 2024-05-11 13:03:57 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2024-05-11 13:03:57 clustering_diarizer:309] Split long audio file to avoid CUDA memory issue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "splitting manifest: 100%|██████████| 1/1 [00:06<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:04:04 classification_models:273] Perform streaming frame-level VAD\n",
      "[NeMo I 2024-05-11 13:04:04 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-05-11 13:04:04 collections:446] Dataset loaded with 32 items, total duration of  0.44 hours.\n",
      "[NeMo I 2024-05-11 13:04:04 collections:448] # 32 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vad: 100%|██████████| 32/32 [00:40<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:04:44 clustering_diarizer:250] Generating predictions with overlapping input segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:04:59 clustering_diarizer:262] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating speech segments: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:05:00 clustering_diarizer:287] Subsegmentation for embedding extraction: scale0, /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/subsegments_scale0.json\n",
      "[NeMo I 2024-05-11 13:05:00 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2024-05-11 13:05:00 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-05-11 13:05:00 collections:446] Dataset loaded with 1593 items, total duration of  0.58 hours.\n",
      "[NeMo I 2024-05-11 13:05:00 collections:448] # 1593 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/5] extract embeddings: 100%|██████████| 25/25 [01:22<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:06:22 clustering_diarizer:389] Saved embedding files to /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings\n",
      "[NeMo I 2024-05-11 13:06:22 clustering_diarizer:287] Subsegmentation for embedding extraction: scale1, /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/subsegments_scale1.json\n",
      "[NeMo I 2024-05-11 13:06:23 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2024-05-11 13:06:23 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-05-11 13:06:23 collections:446] Dataset loaded with 1929 items, total duration of  0.60 hours.\n",
      "[NeMo I 2024-05-11 13:06:23 collections:448] # 1929 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2/5] extract embeddings: 100%|██████████| 31/31 [01:22<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:07:45 clustering_diarizer:389] Saved embedding files to /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings\n",
      "[NeMo I 2024-05-11 13:07:45 clustering_diarizer:287] Subsegmentation for embedding extraction: scale2, /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/subsegments_scale2.json\n",
      "[NeMo I 2024-05-11 13:07:45 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2024-05-11 13:07:45 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-05-11 13:07:45 collections:446] Dataset loaded with 2404 items, total duration of  0.62 hours.\n",
      "[NeMo I 2024-05-11 13:07:45 collections:448] # 2404 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3/5] extract embeddings: 100%|██████████| 38/38 [01:22<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:09:08 clustering_diarizer:389] Saved embedding files to /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings\n",
      "[NeMo I 2024-05-11 13:09:08 clustering_diarizer:287] Subsegmentation for embedding extraction: scale3, /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/subsegments_scale3.json\n",
      "[NeMo I 2024-05-11 13:09:09 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2024-05-11 13:09:09 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-05-11 13:09:09 collections:446] Dataset loaded with 3248 items, total duration of  0.64 hours.\n",
      "[NeMo I 2024-05-11 13:09:09 collections:448] # 3248 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4/5] extract embeddings: 100%|██████████| 51/51 [01:22<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:10:32 clustering_diarizer:389] Saved embedding files to /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings\n",
      "[NeMo I 2024-05-11 13:10:32 clustering_diarizer:287] Subsegmentation for embedding extraction: scale4, /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/subsegments_scale4.json\n",
      "[NeMo I 2024-05-11 13:10:32 clustering_diarizer:343] Extracting embeddings for Diarization\n",
      "[NeMo I 2024-05-11 13:10:32 collections:445] Filtered duration for loading collection is  0.00 hours.\n",
      "[NeMo I 2024-05-11 13:10:32 collections:446] Dataset loaded with 4949 items, total duration of  0.67 hours.\n",
      "[NeMo I 2024-05-11 13:10:32 collections:448] # 4949 files loaded accounting to # 1 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5/5] extract embeddings: 100%|██████████| 78/78 [01:44<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:18 clustering_diarizer:389] Saved embedding files to /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:12:18 speaker_utils:464] cuda=False, using CPU for eigen decomposition. This might slow down the clustering process.\n",
      "clustering: 100%|██████████| 1/1 [00:08<00:00,  8.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:26 clustering_diarizer:464] Outputs are saved in /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:12:26 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:26 msdd_models:960] Loading embedding pickle file of scale:0 at /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings/subsegments_scale0_embeddings.pkl\n",
      "[NeMo I 2024-05-11 13:12:26 msdd_models:960] Loading embedding pickle file of scale:1 at /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings/subsegments_scale1_embeddings.pkl\n",
      "[NeMo I 2024-05-11 13:12:26 msdd_models:960] Loading embedding pickle file of scale:2 at /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings/subsegments_scale2_embeddings.pkl\n",
      "[NeMo I 2024-05-11 13:12:26 msdd_models:960] Loading embedding pickle file of scale:3 at /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings/subsegments_scale3_embeddings.pkl\n",
      "[NeMo I 2024-05-11 13:12:26 msdd_models:960] Loading embedding pickle file of scale:4 at /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/embeddings/subsegments_scale4_embeddings.pkl\n",
      "[NeMo I 2024-05-11 13:12:26 msdd_models:938] Loading cluster label file from /mnt/net/i2x256-ai03/hotel/phit/personal/asr/temp_outputs/speaker_outputs/subsegments_scale4_cluster.label\n",
      "[NeMo I 2024-05-11 13:12:27 collections:761] Filtered duration for loading collection is 0.000000.\n",
      "[NeMo I 2024-05-11 13:12:27 collections:764] Total 1 session files loaded accounting to # 1 audio clips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:28 msdd_models:1403]      [Threshold: 0.7000] [use_clus_as_main=False] [diar_window=50]\n",
      "[NeMo I 2024-05-11 13:12:28 speaker_utils:93] Number of files to diarize: 1\n",
      "[NeMo I 2024-05-11 13:12:28 speaker_utils:93] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:12:28 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:28 speaker_utils:93] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:12:28 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:28 speaker_utils:93] Number of files to diarize: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-11 13:12:28 der:185] Check if each ground truth RTTMs were present in the provided manifest file. Skipping calculation of Diariazation Error Rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-11 13:12:28 msdd_models:1431]   \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Initialize NeMo MSDD diarization model\n",
    "msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(\"cpu\") # cuda \n",
    "msdd_model.diarize()\n",
    "\n",
    "del msdd_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Spekers to Sentences According to Timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Reading timestamps <> Speaker Labels mapping\n",
    "\n",
    "speaker_ts = []\n",
    "with open(os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\"), \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line_list = line.split(\" \")\n",
    "        s = int(float(line_list[5]) * 1000)\n",
    "        e = s + int(float(line_list[8]) * 1000)\n",
    "        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
    "\n",
    "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realligning Speech segments using Punctuation\n",
    "---\n",
    "This code provides a method for disambiguating speaker labels in cases where a sentence is split between two different speakers. It uses punctuation markings to determine the dominant speaker for each sentence in the transcription.\n",
    "```\n",
    "Speaker A: It's got to come from somewhere else. Yeah, that one's also fun because you know the lows are\n",
    "Speaker B: going to suck, right? So it's actually it hits you on both sides.\n",
    "```\n",
    "For example, if a sentence is split between two speakers, the code takes the mode of speaker labels for each word in the sentence, and uses that speaker label for the whole sentence. This can help to improve the accuracy of speaker diarization, especially in cases where the Whisper model may not take fine utterances like \"hmm\" and \"yeah\" into account, but the Diarization Model (Nemo) may include them, leading to inconsistent results.\n",
    "\n",
    "The code also handles cases where one speaker is giving a monologue while other speakers are making occasional comments in the background. It ignores the comments and assigns the entire monologue to the speaker who is speaking the majority of the time. This provides a robust and reliable method for realigning speech segments to their respective speakers based on punctuation in the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1096307/416299889.py\", line 7, in <module>\n",
      "    labled_words = punct_model.predict(words_list)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/deepmultilingualpunctuation/punctuationmodel.py\", line 47, in predict\n",
      "    result = self.pipe(text)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/transformers/pipelines/token_classification.py\", line 248, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/space/hotel/phit/miniconda3/envs/speech/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "source": [
    "if language in punct_model_langs:\n",
    "    # restoring punctuation in the transcript to help realign the sentences\n",
    "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
    "\n",
    "    words_list = list(map(lambda x: x[\"word\"], wsm))\n",
    "\n",
    "    labled_words = punct_model.predict(words_list)\n",
    "\n",
    "    ending_puncts = \".?!\"\n",
    "    model_puncts = \".,;:!?\"\n",
    "\n",
    "    # We don't want to punctuate U.S.A. with a period. Right?\n",
    "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
    "\n",
    "    for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
    "        word = word_dict[\"word\"]\n",
    "        if (\n",
    "            word\n",
    "            and labeled_tuple[1] in ending_puncts\n",
    "            and (word[-1] not in model_puncts or is_acronym(word))\n",
    "        ):\n",
    "            word += labeled_tuple[1]\n",
    "            if word.endswith(\"..\"):\n",
    "                word = word.rstrip(\".\")\n",
    "            word_dict[\"word\"] = word\n",
    "\n",
    "else:\n",
    "    logging.warning(\n",
    "        f\"Punctuation restoration is not available for {language} language. Using the original punctuation.\"\n",
    "    )\n",
    "\n",
    "wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
    "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/space/hotel/phit/personal/asr/data/raw/audios/Y8tlFLIjyMU', '.mp3')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import config as cfg\n",
    "_id = os.path.splitext(audio_path)[0].split(\"/\")[-1]\n",
    "with open(f\"{cfg.RESULT_PATH}/{_id}.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    get_speaker_aware_transcript(ssm, f)\n",
    "\n",
    "with open(f\"{cfg.RESULT_PATH}/{_id}.srt\", \"w\", encoding=\"utf-8-sig\") as srt:\n",
    "    write_srt(ssm, srt)\n",
    "\n",
    "try:\n",
    "    cleanup(temp_path)\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
